{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Trainer_all_smooth_and_zeroed.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PorkPy/LSTM-Force-Predictor/blob/master/Trainer_all_smooth_and_zeroed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtEIpTbrfxxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mzCKBtLA-zOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "% reset -f\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import math\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch import nn, optim\n",
        "import random\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "from scipy.stats import norm\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from subprocess import call\n",
        "import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "## Set random seed for numpy and Torch\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKX3q9H0_RZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('__Python VERSION:', sys.version)\n",
        "print('__pyTorch VERSION:', torch.__version__)\n",
        "print('__CUDA VERSION')\n",
        "# call([\"nvcc\", \"--version\"]) does not work\n",
        "! nvcc --version\n",
        "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
        "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
        "print('__Devices')\n",
        "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
        "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
        "print('Available devices ', torch.cuda.device_count())\n",
        "print('Current cuda device ', torch.cuda.current_device())\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5WXvVoh-zOu",
        "colab_type": "text"
      },
      "source": [
        "Check CUDA is available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJf9cG7o-zOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ryldrf8Qhv0",
        "colab_type": "text"
      },
      "source": [
        "User defined params\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeB6Bzi-m2UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35dozZZC3Nut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### MODEL PARAMETERS ###\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_num  = '90'        ## model number to save new models with\n",
        "params     = '81_v40' ## which model params to load...\n",
        "warm_start = False       ## ...and if to load them.\n",
        "model_dir  = 'model90'   ## Directory for specific model being trained. \n",
        "seq_length = 1000         ## The length of the trajectory sequence batch.\n",
        "epochs     = 201         ## Number of full passes through the whole dataset.\n",
        "hidden     = 60          ## Number of nodes in the LSTM layers.\n",
        "lr         = 0.01       ## Learning rate.\n",
        "feature_num   =  4          ## 4 features for joint data, 6 features for cartesian data.\n",
        "fc         = 1           ## Number of fully connected layers. 1 or 2.\n",
        "path       = f\"/content/drive/My Drive/PhD/PhD/lstm/{model_dir}/\" ## Save directory.\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "## I increased the batch size and lr by 1 order.\n",
        "\n",
        "def model_number():\n",
        "    return model_num\n",
        "\n",
        "def load_params():\n",
        "    return params\n",
        "\n",
        "def model_directory():\n",
        "    return model_dir\n",
        "\n",
        "def get_seq_length():\n",
        "    return seq_length\n",
        "\n",
        "def get_epochs():\n",
        "    return epochs\n",
        "\n",
        "def get_warm_start():\n",
        "    return warm_start\n",
        "\n",
        "def get_hidden():\n",
        "    return hidden\n",
        "\n",
        "def get_lr():\n",
        "    return lr\n",
        "\n",
        "def get_path():\n",
        "    return path\n",
        "\n",
        "def get_features():\n",
        "    return feature_num\n",
        "\n",
        "def get_fc():\n",
        "    return fc\n",
        "\n",
        "## Dictionary with which to save paramers.\n",
        "param = {'Model Num':model_num, \n",
        "          'Seq Length': seq_length,\n",
        "          'Epochs': epochs,\n",
        "          'Warm Start': warm_start,\n",
        "          'Hidden Size': hidden,\n",
        "          'Learning Rate': lr,\n",
        "          'Data': 'smooth 3 force data',\n",
        "          'features': feature_num, \n",
        "          'Num LSTM Layers':2,\n",
        "          'Num FC Layers':1}\n",
        "\n",
        "## Create new directory in perent directory to save parameters.\n",
        "try:\n",
        "    os.makedirs(path)\n",
        "except OSError:\n",
        "    print (\"Creation of the directory %s failed\" % path)\n",
        "else:\n",
        "    print (\"Successfully created the directory %s \" % path)\n",
        "\n",
        "\n",
        "## create a pandas data frame of the model parameters and save to csv.\n",
        "param = pd.DataFrame(param, index=[0])\n",
        "param.to_csv(path + \"lstm_params.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "O7ZKYszy-zQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def tests(model_name):\n",
        "    \n",
        "    seq_len = get_seq_length()\n",
        "    model_name = model_name\n",
        "    model_dir = model_directory()\n",
        "    path = get_path()\n",
        "    features_num = get_features()\n",
        "\n",
        "    \n",
        "    ## Create new directory in perent directory\n",
        "    path = path + f\"{model_name}/\"\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError:\n",
        "        print (\"Creation of the directory %s failed\" % path)\n",
        "    else:\n",
        "        print (\"Successfully created the directory %s \" % path)\n",
        "\n",
        "    stats_list = []\n",
        "    pdf = PdfPages(path + f\"testing_traj_pics_{model_name}.pdf\")\n",
        "    fig = plt.figure()\n",
        "\n",
        "    for traj in range(len(test_batches)):\n",
        "        Xtest, ytest, jim = get_test_batch(traj)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            model.reset_hidden_state()\n",
        "            \n",
        "            x = iter(Xtest)\n",
        "            test_seq = Xtest[0].reshape(-1,seq_len,features_num)#.reshape(1,200,4) # input first sequence from trajectory/batch\n",
        "            preds = [] # create a list to store predictions.\n",
        "            for i in range(len(Xtest)): # for each sequence i in the trajectory,\n",
        "                y_test_pred = model(test_seq).to(device)# send sequence to model,\n",
        "                pred = torch.flatten(y_test_pred).cpu() # reshape the model output,\n",
        "                preds.append(np.asarray(pred)) # and append to the list of predictions - preds.\n",
        "                new_seq = next(x).reshape(-1,seq_len,features_num)#.reshape(1,200,4) # Change sequence to the next one in the list.\n",
        "                test_seq = torch.cuda.FloatTensor(new_seq).view(1, seq_len, -1) # change sequence to a torch Tensor\n",
        "                #model.reset_hidden_state()\n",
        "                \n",
        "        ## rescale the output predictions\n",
        "        preds = target_scaler.inverse_transform(preds).reshape(-1,3)\n",
        "        ## Vector summation - the vector sum of the 3 output predictions\n",
        "        force_vec = np.sqrt((preds[:,0]**2)+(preds[:,1]**2)+(preds[:,2]**2))\n",
        "        \n",
        "        preds = force_vec ## reset name to comply with existing code.\n",
        "        #display(force_vec)\n",
        "\n",
        "        # Mean Absolute Error\n",
        "        MAE_list = []\n",
        "        for i,j in zip(preds, ytest):\n",
        "            error = np.abs(i-j)\n",
        "            MAE_list.append(error)\n",
        "        MAE = float(\"{:.3f}\".format(np.mean(MAE_list)))\n",
        "        #print(\"MAE\",\"{:.3f}\".format(MAE),'N')\n",
        "\n",
        "        # Coefficient of Variance\n",
        "        mean = np.mean(data.iloc[:,-1]) # mean of all dependent variables.\n",
        "        cov_list = []\n",
        "        for i,j in zip(preds, ytest):\n",
        "            sq_dev = (i-j)**2\n",
        "            cov_list.append(sq_dev)    \n",
        "        MSD = np.mean(cov_list) # mean square deviation\n",
        "        RMSD = np.sqrt(MSD) # root mean square deviation\n",
        "        cov = RMSD/mean # coefficient of variance\n",
        "        RMSD = float(\"{:.3f}\".format(RMSD))\n",
        "        cov =  float(\"{:.3f}\".format(cov))\n",
        "        #print(\"COV:\",\"{:.3f}\".format(cov))\n",
        "        \n",
        "    \n",
        "        my_dict = {'Trajectory':traj,\n",
        "                'MAE': MAE, \n",
        "                'RMSD':RMSD,\n",
        "                'cov': cov, # Used to normalise the RMSD accross all the data\n",
        "        }\n",
        "        stats_list.append(my_dict)\n",
        "\n",
        "        def lighten_color(color, amount=0.5):\n",
        "            import matplotlib.colors as mc\n",
        "            import colorsys\n",
        "            try:\n",
        "                c = mc.cnames[color]\n",
        "            except:\n",
        "                c = color\n",
        "            c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n",
        "            return colorsys.hls_to_rgb(c[0], max(0, min(1, amount * c[1])), c[2])\n",
        "\n",
        "        # Plot forces\n",
        "        predicted_cases = preds\n",
        "        true_cases = ytest\n",
        "        # Add title and axis names\n",
        "        plt.title(f'Force Trajectory {traj}');\n",
        "        plt.xlabel('Sample num');\n",
        "        plt.ylabel('Force (N)');\n",
        "        plt.tight_layout();\n",
        "        #plt.plot(jim,label='Sequence')\n",
        "        plt.ylim(0, 70)\n",
        "\n",
        "        plt.plot(true_cases, color=lighten_color('b', 1.8), linewidth=3.0, label='Real Force');\n",
        "        \n",
        "        plt.plot(predicted_cases, color=lighten_color('b', 1.2), linewidth=1.0, label='Predicted Force');\n",
        "        \n",
        "        plt.legend(loc=2, prop={'size': 6})\n",
        "\n",
        "        # save the current figure\n",
        "        pdf.savefig(fig);\n",
        "        # destroy the current figure\n",
        "        plt.clf()\n",
        "\n",
        "    # close the object\n",
        "    # fig = plt.figure()\n",
        "    # plt.plot(train_hist, label=\"Training loss\")\n",
        "    # plt.plot(test_hist, label=\"Test loss\")\n",
        "    # plt.legend();\n",
        "    # pdf.savefig(fig)\n",
        "    # plt.clf\n",
        "    pdf.close()\n",
        "    stats_list = pd.DataFrame(stats_list)\n",
        "    return stats_list\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def stats(stats_list2, model_name):\n",
        "    \n",
        "    \n",
        "    #display(stats_list2)\n",
        "    #display(stats_list2['MAE'])\n",
        "\n",
        "    mean_list = {\n",
        "                'MAE' :float(\"{:.3f}\".format(np.mean(stats_list2['MAE']))),\n",
        "                'RMSD':float(\"{:.3f}\".format(np.mean(stats_list2['RMSD']))),\n",
        "                'cov' :float(\"{:.3f}\".format(np.mean(stats_list2['cov'])))\n",
        "    }\n",
        "\n",
        "    std_dev = {\n",
        "                'MAE' :float(\"{:.3f}\".format(np.std(stats_list2['MAE']))),\n",
        "                'RMSD':float(\"{:.3f}\".format(np.std(stats_list2['RMSD']))),\n",
        "                'cov' :float(\"{:.3f}\".format(np.std(stats_list2['cov'])))\n",
        "    }\n",
        "\n",
        "    max_list = {\n",
        "                'MAE' :float(stats_list2['MAE'].max()),\n",
        "                'RMSD':float(stats_list2['RMSD'].max()),\n",
        "                'cov' :float(stats_list2['cov'].max())\n",
        "    }\n",
        "\n",
        "    stats_list2 = stats_list2.append(mean_list, ignore_index=True).fillna('Grand Mean')\n",
        "    stats_list2 = stats_list2.append(std_dev, ignore_index=True).fillna('Standard Dev')\n",
        "    stats_list2 = stats_list2.append(max_list, ignore_index=True).fillna('Max Value')\n",
        "\n",
        "    #display(stats_list2)\n",
        "    path = get_path()\n",
        "    \n",
        "    ## Create new directory in perent directory\n",
        "    path = path + f\"{model_name}/\"\n",
        "    model_name = model_name\n",
        "\n",
        "    stats_list2.to_csv(path + f\"lstm_model_metrics_{model_name}.csv\", index=False)\n",
        "    return stats_list2\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def gauss_plot(stats_list2, name, error_type, num):\n",
        "    import matplotlib.pyplot as plt\n",
        "    model_name = name\n",
        "    model_dir = model_directory()\n",
        "    path = get_path()\n",
        "    path = path + f\"{model_name}/\"\n",
        "\n",
        "    error = error_type\n",
        "    pdf = PdfPages(path + f\"gauss_pic_{error}.pdf\")\n",
        "    fig = plt.figure()\n",
        "    \n",
        "    # define constants\n",
        "    mu = np.mean(stats_list2.iloc[:-3,num]) \n",
        "    sigma = np.sqrt(np.var(stats_list2.iloc[:-3,num]))\n",
        "    x1 = np.min(stats_list2.iloc[:-3,num])\n",
        "    x2 = np.max(stats_list2.iloc[:-3,num])\n",
        "    # print(mu)\n",
        "    # print(sigma)\n",
        "    # print(x1)\n",
        "    # print(x2)\n",
        "\n",
        "    # calculate the z-transform\n",
        "    z1 = ( x1 - mu ) / sigma\n",
        "    z2 = ( x2 - mu ) / sigma\n",
        "\n",
        "    x = np.arange(z1, z2, 0.001) # range of x in spec\n",
        "    x_all = np.arange(-10, 10, 0.001) # entire range of x, both in and out of spec\n",
        "    # mean = 0, stddev = 1, since Z-transform was calculated\n",
        "    y = norm.pdf(x,0,1);\n",
        "    y2 = norm.pdf(x_all,0,1);\n",
        "\n",
        "    # build the plot\n",
        "    fig, ax = plt.subplots(figsize=(9,6));\n",
        "    plt.style.use('fivethirtyeight');\n",
        "    ax.plot(x_all,y2);\n",
        "\n",
        "    ax.fill_between(x,y,0, alpha=0.3, color='b');\n",
        "    ax.fill_between(x_all,y2,0, alpha=0.1);\n",
        "    ax.set_xlim([-4,4]);\n",
        "    ax.set_xlabel('# of Standard Deviations Outside the Mean');\n",
        "    ax.set_yticklabels([]);\n",
        "    ax.set_title(f'{model_name} {error} Std Dev');\n",
        "\n",
        "    plt.savefig('normal_curve.png', dpi=72, bbox_inches='tight');\n",
        "    plt.grid(True);\n",
        "    #plt.show()\n",
        "\n",
        "    # save the current figure\n",
        "    pdf.savefig(fig);\n",
        "    # destroy the current figure\n",
        "    plt.clf()\n",
        "\n",
        "    # close the object\n",
        "    pdf.close()\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def prob_dist(stats_list2, name, error_type, num):    \n",
        "    model_name = name\n",
        "    model_dir = model_directory()\n",
        "    path = get_path()\n",
        "    path = path + f\"{model_name}/\"\n",
        "\n",
        "    error = error_type\n",
        "    pdf = PdfPages(path + f\"prob_dist_pic_{error}.pdf\")\n",
        "    fig = plt.figure()\n",
        "\n",
        "    import seaborn as sns\n",
        "    sns.distplot(stats_list2.iloc[:-3,num], color=\"darkslategrey\");\n",
        "    plt.xlabel(\"Force [newtons]\", labelpad=14);\n",
        "    plt.ylabel(\"Probability of Occurence\", labelpad=14);\n",
        "    plt.title(f\"Probability Distribution of {error}\", fontsize=20);\n",
        "    #plt.show()\n",
        "    # save the current figure\n",
        "    pdf.savefig(fig);\n",
        "    # destroy the current figure\n",
        "    plt.clf()\n",
        "    plt.close('all') ## added this due to runtime warning, more than 20 figs open\n",
        "    # close the object\n",
        "    pdf.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2BXoG-jlHcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_runner(name):   \n",
        "    stats_df = tests(name) # Run tests on testing data and save generated plots to Google Drive\n",
        "    stats(stats_df, name) # Record stats and save to Google Drive\n",
        "    for i in range(1,4): # 1 to 3 = the colunms in the stats_list DataFrame\n",
        "        if i ==1:\n",
        "            error_type = 'MAE' # mean absolur error\n",
        "        elif i == 2:\n",
        "            error_type = 'RMSE' # root mean squared error\n",
        "        elif i == 3:\n",
        "            error_type = 'cov' # coefficient of variance\n",
        "\n",
        "        #prob_dist(stats_df, name, error_type, i) # Gen prob_dist and save to GD\n",
        "        \n",
        "        #gauss_plot(stats_df, name, error_type, i) # Gen Gauss plots and save to GD\n",
        "    print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSq64dXK-zO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ForcePredictor(nn.Module):\n",
        "\n",
        "    def __init__(self, n_features, n_hidden, seq_len, n_layers=2, ignore_zero=True):\n",
        "        super(ForcePredictor, self).__init__()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.device(\"cuda:0\")\n",
        "            print(\"Running on the GPU\")\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "            print(\"Running on CPU\")\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "        self.seq_len = seq_len\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "          input_size=n_features,\n",
        "          hidden_size=n_hidden,\n",
        "          num_layers=n_layers,\n",
        "          dropout=0.5)\n",
        "        \n",
        "        fc = get_fc() ## get num of FC layers\n",
        "\n",
        "        if fc == 1:\n",
        "            self.linear1 = nn.Linear(in_features=n_hidden, out_features=3)\n",
        "            \n",
        "        elif fc == 2:\n",
        "            self.linear1 = nn.Linear(in_features=n_hidden, out_features=60)\n",
        "            self.linear2 = nn.Linear(in_features=60, out_features=3)\n",
        "\n",
        "        elif fc == 3:\n",
        "            self.linear1 = nn.Linear(in_features=60, out_features=60)\n",
        "            self.linear2 = nn.Linear(in_features=n_hidden, out_features=60)\n",
        "            self.linear3 = nn.Linear(in_features=60, out_features=3)\n",
        "        \n",
        "    def reset_hidden_state(self):\n",
        "        self.hidden = (\n",
        "            torch.zeros(self.n_layers, self.seq_len, self.n_hidden).to(device),\n",
        "            torch.zeros(self.n_layers, self.seq_len, self.n_hidden).to(device)\n",
        "        )\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        \n",
        "        fc = get_fc() ## get num of FC layers\n",
        "\n",
        "        lstm_out, self.hidden = self.lstm(sequences.view(len(sequences), self.seq_len, -1),self.hidden)\n",
        "        last_time_step = lstm_out.view(self.seq_len, len(sequences), self.n_hidden)[-1]\n",
        "\n",
        "        if fc == 1:\n",
        "            y_pred = self.linear1(last_time_step)\n",
        "\n",
        "        if fc == 2:\n",
        "            y_pred = F.leaky_relu(self.linear1(last_time_step))\n",
        "            y_pred = self.linear3(y_pred)\n",
        "\n",
        "        if fc == 3:\n",
        "            y_pred = F.leaky_relu(self.linear1(last_time_step))\n",
        "            y_pred = F.leaky_relu(self.linear2(y_pred))\n",
        "            y_pred = self.linear3(y_pred)\n",
        "\n",
        "       \n",
        "\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThRgX0Yo-zPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on CPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "894bcCIs-zPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model):\n",
        "    lr = get_lr()\n",
        "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)#0.0007 \n",
        "    print(\"learning rate =\", lr) \n",
        "    num_epochs = get_epochs() #1600 #600\n",
        "    path = get_path()\n",
        "    seq_length = get_seq_length()\n",
        "\n",
        "    start_epoch = 0\n",
        "    warm_start = get_warm_start()\n",
        "    if warm_start == True:      \n",
        "        params = load_params() # model num and version num: 4_v100.\n",
        "        PATH = f\"/content/drive/My Drive/PhD/PhD/lstm/model_params{params}.pt\"     \n",
        "        checkpoint = torch.load(PATH)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        for state in optimizer.state.values():\n",
        "            for k, v in state.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    state[k] = v.cuda()\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        loss = checkpoint['loss']\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_hist = np.zeros(num_epochs + start_epoch)\n",
        "    test_hist = np.zeros(num_epochs + start_epoch)\n",
        "\n",
        "    \n",
        "    losses = []\n",
        "    tot_losses = []\n",
        "    test_losses = []\n",
        "    tot_test_losses = []\n",
        "    for t in range( num_epochs):\n",
        "        \n",
        "        for j in range(len(batches)):\n",
        "            \n",
        "            ## Fetch n samples from each trajectory to train on before updatining network.\n",
        "            for start_seq in range(int(1000/seq_length)): \n",
        "                start_seqx = start_seq*seq_length ## get the next sequence start position\n",
        "\n",
        "                train_data, train_labels, test_data, test_labels = get_batches(j, start_seqx)\n",
        "\n",
        "                if start_seq % int(1000/seq_length) == 0:\n",
        "                    model.reset_hidden_state() ## reset after every trajectory\n",
        "                y_pred = model(train_data)\n",
        "                loss = loss_fn(y_pred.float(), train_labels)\n",
        "                losses.append(loss)\n",
        "                \n",
        "\n",
        "                if test_data is not None:\n",
        "                    with torch.no_grad():\n",
        "                        model.reset_hidden_state() # Requiered because hidden state was influencing the prediction. \n",
        "\n",
        "                        y_test_pred = model(test_data)\n",
        "                        test_loss = loss_fn(y_test_pred.float(), test_labels)\n",
        "                    test_hist[t] = test_loss.item()\n",
        "                    test_losses.append(test_loss)\n",
        "\n",
        "                    ## The loss will look small (<1) but that's because we are not de-scaling the output. \n",
        "                    if t % 10 == 0:  \n",
        "                        print(f'Epoch {t} {j} {start_seq} train loss: {loss.item()} test loss: {test_loss.item()}')\n",
        "                elif t % 10 == 0:\n",
        "                    print(f'Epoch {t} {j} {start_seq} train loss: {loss.item()}')\n",
        "\n",
        "                train_hist[t] = loss.item()\n",
        "\n",
        "                #if j % 5 == 0:\n",
        "                optimizer.zero_grad()\n",
        "                #loss = (sum(losses))/len(losses)\n",
        "                loss.backward()\n",
        "                model = model.to(device)\n",
        "                optimizer.step()\n",
        "                #print(model.linear4.weight.data) # Check weights are being updated.\n",
        "                #losses = []\n",
        "\n",
        "        ## Periodically save model and show training and testing loss\n",
        "        if t % 10 == 0:\n",
        "            print('Saving model', '\\n')\n",
        "            model_num = model_number()\n",
        "            model_save_name = f'model_params{model_num}_v{t}.pt'\n",
        "            torch.save({\n",
        "                'epoch': num_epochs,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss':loss,},\n",
        "                f\"/content/drive/My Drive/PhD/PhD/lstm/{model_save_name}\" \n",
        "            )\n",
        "\n",
        "            ## Show the training and testing losses during execution.\n",
        "            from matplotlib.pyplot import figure\n",
        "            figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "            figure(figsize=(20,4))\n",
        "            plt.plot(losses, label='Training Loss');\n",
        "            plt.plot(test_losses, label='Testing Loss');\n",
        "            plt.legend();\n",
        "            plt.tight_layout();\n",
        "            plt.show()\n",
        "            plt.pause(0.01)\n",
        "            mean_loss = sum(losses)/len(losses)\n",
        "            mean_test_loss = sum(test_losses)/len(test_losses)\n",
        "            print(\"\\n Average Loss\")\n",
        "            print(mean_loss.item())\n",
        "            print(\"\\n Average Test Loss\")\n",
        "            print(mean_test_loss.item(),'\\n')\n",
        "            diff = mean_test_loss - mean_loss\n",
        "            print(\"Difference Between Training and Testing Losses\")\n",
        "            print(diff.item(),'\\n')\n",
        "            tot_losses.append(mean_loss)\n",
        "            tot_test_losses.append(mean_test_loss)\n",
        "        \n",
        "\n",
        "            name = f'model{model_num}_v{t}'\n",
        "            test_runner(name)\n",
        "            model.train() ## just in case it was left in .eval() mode.\n",
        "\n",
        "            model_dir = model_directory()\n",
        "\n",
        "            pdf = PdfPages(f\"/content/drive/My Drive/PhD/PhD/lstm/{model_dir}/loss.pdf\")\n",
        "            fig = plt.figure();\n",
        "\n",
        "            plt.plot(tot_losses, label='Training Loss');\n",
        "            plt.plot(tot_test_losses, label='Testing Loss');\n",
        "            plt.xlabel(\"100 epochs\", labelpad=14);\n",
        "            plt.ylabel(\"Loss\", labelpad=14);\n",
        "            plt.title(f\"Training and Testing Losses {model_dir}\", fontsize=20);\n",
        "            plt.grid(True);\n",
        "            plt.tight_layout();\n",
        "            plt.legend();\n",
        "\n",
        "            #plt.show()\n",
        "            # save the current figure\n",
        "            pdf.savefig(fig);\n",
        "            # destroy the current figure\n",
        "            plt.clf()\n",
        "            # close the object\n",
        "            pdf.close()\n",
        "\n",
        "        losses = []\n",
        "        test_losses = []\n",
        "\n",
        "    return model.eval(), train_hist, test_hist, optimizer, t, loss_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4nlT0C-9fUv",
        "colab_type": "text"
      },
      "source": [
        "This section automatically selects which dataset to download based on the feature number selection made at the start. 4 features for the joint data and 6 features for the Cartesian data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtXzj7tF-zPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_num = get_features()\n",
        "if feature_num == 4:\n",
        "    url = 'https://raw.githubusercontent.com/PorkPy/LSTM-Force-Predictor/master/80k_data/mean_force_components_and_vectorsum_zeroed_columns.csv'\n",
        "else:\n",
        "    url = 'https://raw.githubusercontent.com/PorkPy/LSTM-Force-Predictor/master/80k_data/four_joints_and_three_force.csv'\n",
        "\n",
        "data = pd.read_csv(url)\n",
        "main_seq = data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTNvxhJxohyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(data.head(21))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I218Ofy0ip-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = data.iloc[:,:4]\n",
        "display(features)\n",
        "\n",
        "feature_scaler = StandardScaler()\n",
        "features = feature_scaler.fit_transform(features)\n",
        "\n",
        "targets = data.iloc[:,4:7]\n",
        "display(targets)\n",
        "\n",
        "target_scaler = StandardScaler()\n",
        "targets = target_scaler.fit_transform(targets)\n",
        "\n",
        "force_vec = pd.DataFrame(data.iloc[:,-1])\n",
        "\n",
        "features = pd.DataFrame(features)\n",
        "targets = pd.DataFrame(targets)\n",
        "data = pd.concat([targets, features, force_vec], axis=1)\n",
        "data.columns = [['Fx', 'Fy', 'Fz', 'joint_0', 'joint_2', 'joint_4', 'joint_5', 'force mean']]\n",
        "display(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "IGY4CFZ9-zP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n=1000  ## num samples per trajectory/sequence.\n",
        "batchesx = [data[i:i + n] for i in range(0, len(data), n)] ## a list comprehension to build the data batches.\n",
        "print(len(batchesx))\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(batchesx)\n",
        "\n",
        "\n",
        "batches = batchesx[:60] ## Training batches up to the 60th sequence/trajectory.\n",
        "val_batches = batchesx[60:] ## Validation batches starting from the 60th sequence.\n",
        "\n",
        "## Append extra validation batches to even the number training and validation batches.\n",
        "## This is because the training loop performs a validation test on each iteration\n",
        "## and so always needs something to validate against.  \n",
        "while len(val_batches) < len(batches): \n",
        "    for i in batchesx[60:]:\n",
        "        val_batches.append(i)\n",
        "random.shuffle(val_batches)\n",
        "\n",
        "test_batches = batchesx[60:] ## Testing batches, same as validation batches, without the appendages.\n",
        "print(len(batches), len(val_batches), len(test_batches))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WabAxpcY-zP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_batch(batch_number):\n",
        "    \n",
        "    seq_size = get_seq_length() ## 1000 for testing 50-100 for training\n",
        "    features_num = get_features()\n",
        "\n",
        "    X_test = []\n",
        "\n",
        "    data = test_batches[batch_number].reset_index(drop=True)\n",
        "    data= data[:seq_size]\n",
        "\n",
        "    if features_num == 4:\n",
        "        features = data[['joint_0', 'joint_2', 'joint_4', 'joint_5']]\n",
        "    else:\n",
        "        features = data[['x', 'y']]#, 'z', 'Rx', 'Ry', 'Rz']]\n",
        "    features = np.asarray(features)\n",
        "\n",
        "    \n",
        "\n",
        "    targets = data.iloc[:,:3]\n",
        "    targets = np.asarray(targets)\n",
        "    targets = targets.reshape(-1,3)\n",
        "    target = data.iloc[:,-1]\n",
        "    \n",
        "    for i in range(len(features)):           \n",
        "   \n",
        "        X =(features[:i+1])\n",
        "        an_array = np.array(X)\n",
        "        shape = np.shape(X)\n",
        "        temp = np.zeros((seq_size, features_num))\n",
        "        temp[(seq_size-shape[0]):,:shape[1]] = an_array\n",
        "        X_test.append(temp)\n",
        "        y_test = target\n",
        "\n",
        "    ## We need a scaler object to rescale the output predictions\n",
        "    ## which are scaled as a result of the scaled input features.\n",
        "    ## We are not actually scaling the targets here, just using it as a reference.\n",
        "    \n",
        "\n",
        "    #y_test = target ## reasign y_test now the scaler object has been created. Now, y_test is the same as it was relative to all the testing metrics.\n",
        "    \n",
        "    X_test = torch.cuda.FloatTensor(X_test)\n",
        "    #y_test = torch.cuda.FloatTensor(y_test)\n",
        "    #print(data, '\\n')\n",
        "    #print( X_test.shape, y_test.shape)\n",
        "    jim = target\n",
        "    return(X_test, y_test, jim)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nehzUS3F-zQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(batch_num, start_seq):  \n",
        "    \n",
        "    seq_size = get_seq_length() # 1000 = full trajectories\n",
        "    features_num = get_features()\n",
        "\n",
        "    # random.seed(batch_num)\n",
        "    # random.shuffle(batches) # ive turned this off to test new cleaned data\n",
        "\n",
        "    # Randomise the fetching of new data to break the corrolation of training.\n",
        "    #print(batch_num)\n",
        "    #print(type(batches[batch_num])) \n",
        "    data = batches[batch_num].reset_index(drop=True)\n",
        "    data= data[start_seq:seq_size+start_seq]\n",
        "    ################################################\n",
        "\n",
        "    X_train = []\n",
        "    X_test = []\n",
        "    \n",
        "    if features_num == 4:\n",
        "        features = data[['joint_0', 'joint_2', 'joint_4', 'joint_5']]\n",
        "    else:\n",
        "        features = data[['x', 'y']]#, 'z', 'Rx', 'Ry', 'Rz']]\n",
        "    features = np.asarray(features)\n",
        "\n",
        "\n",
        "    targets = data.iloc[:,:3]\n",
        "    targets = np.asarray(targets)\n",
        "    targets = targets.reshape(-1,3)\n",
        "\n",
        "\n",
        "    for i in range(len(features)):           \n",
        "        \n",
        "        np.random.seed(42)\n",
        "       \n",
        "        X =(features[:i+1])\n",
        "        an_array = np.array(X)\n",
        "        shape = np.shape(X)\n",
        "        temp = np.zeros((seq_size, features_num))\n",
        "        temp[(seq_size-shape[0]):,:shape[1]] = an_array\n",
        "        X_train.append(temp)\n",
        "        y_train = targets\n",
        "    \n",
        "    ###############################\n",
        "    \n",
        "   \n",
        "    ################################\n",
        "    data = val_batches[batch_num].reset_index(drop=True)\n",
        "    data= data[start_seq:seq_size+start_seq]\n",
        "\n",
        "    if features_num == 4:\n",
        "        features = data[['joint_0', 'joint_2', 'joint_4', 'joint_5']]\n",
        "    else:\n",
        "        features = data[['x', 'y']]#, 'z', 'Rx', 'Ry', 'Rz']]\n",
        "    features = np.asarray(features)\n",
        "\n",
        "\n",
        "    targets = data.iloc[:,:3]\n",
        "    targets = np.asarray(targets)\n",
        "    targets = targets.reshape(-1,3)\n",
        "    \n",
        "    for i in range(len(features)):           \n",
        "   \n",
        "        X =(features[:i+1])\n",
        "        an_array = np.array(X)\n",
        "        shape = np.shape(X)\n",
        "        temp = np.zeros((seq_size, features_num))\n",
        "        temp[(seq_size-shape[0]):,:shape[1]] = an_array\n",
        "        X_test.append(temp)\n",
        "        y_test = targets\n",
        "\n",
        " #############################################\n",
        "\n",
        "\n",
        "###############################################\n",
        "\n",
        "    X_train = torch.cuda.FloatTensor(X_train) # Change data to tensors\n",
        "    y_train = torch.cuda.FloatTensor(y_train)\n",
        "    X_test = torch.cuda.FloatTensor(X_test)\n",
        "    y_test = torch.cuda.FloatTensor(y_test)\n",
        "    \n",
        "    #del targets, features, data, temp, an_array\n",
        "    \n",
        "    #print(data)\n",
        "    #print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "    return(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZE1XwbFV5jO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_batches(batch_num, start_seq):  \n",
        "    \n",
        "#     seq_size = get_seq_length() # 1000 = full trajectories\n",
        "#     features_num = get_features()\n",
        "\n",
        "#     # random.seed(batch_num)\n",
        "#     # random.shuffle(batches) # ive turned this off to test new cleaned data\n",
        "\n",
        "#     # Randomise the fetching of new data to break the corrolation of training.\n",
        "#     #print(batch_num)\n",
        "#     #print(type(batches[batch_num])) \n",
        "#     data = batches[batch_num].reset_index(drop=True)\n",
        "#     data= data[start_seq:seq_size+start_seq]\n",
        "#     ################################################\n",
        "\n",
        "#     X_train = []\n",
        "#     X_test = []\n",
        "    \n",
        "#     if features_num == 4:\n",
        "#         features = data[['joint_0', 'joint_2', 'joint_4', 'joint_5']]\n",
        "#     else:\n",
        "#         features = data[['x', 'y', 'z', 'Rx', 'Ry', 'Rz']]\n",
        "#     features = np.asarray(features)\n",
        "\n",
        "#     scaler2 = StandardScaler()\n",
        "#     #joint_scaler = np.asarray(features)\n",
        "#     #joint_scaler = joint_scaler.reshape(-1, features_num)\n",
        "#     features = scaler2.fit_transform(features) \n",
        "#     #features = scaler2.transform(features)\n",
        "\n",
        "#     targets = data.iloc[:,:3]\n",
        "#     targets = np.asarray(targets)\n",
        "#     targets = targets.reshape(-1,3)\n",
        "\n",
        "\n",
        "#     for i in range(len(features)):           \n",
        "        \n",
        "#         np.random.seed(42)\n",
        "       \n",
        "#         X =(features[:i+1])\n",
        "#         an_array = np.array(X)\n",
        "#         shape = np.shape(X)\n",
        "#         temp = np.zeros((seq_size, features_num))\n",
        "#         temp[(seq_size-shape[0]):,:shape[1]] = an_array\n",
        "#         X_train.append(temp)\n",
        "#         y_train = targets\n",
        "    \n",
        "#     ###############################\n",
        "    \n",
        "#     scaler = StandardScaler()\n",
        "#     #force_scaler = np.asarray(y_train)\n",
        "#     #force_scaler = force_scaler.reshape(-1,3)\n",
        "#     y_train = scaler.fit_transform(y_train)    \n",
        "#     #y_train = scaler.transform(y_train)\n",
        "\n",
        "    \n",
        "    \n",
        "#     ################################\n",
        "#     data = val_batches[batch_num].reset_index(drop=True)\n",
        "#     data= data[start_seq:seq_size+start_seq]\n",
        "\n",
        "#     if features_num == 4:\n",
        "#         features = data[['joint_0', 'joint_2', 'joint_4', 'joint_5']]\n",
        "#     else:\n",
        "#         features = data[['x', 'y', 'z', 'Rx', 'Ry', 'Rz']]\n",
        "#     features = np.asarray(features)\n",
        "\n",
        "#     #scaler2 = StandardScaler()\n",
        "#     #joint_scaler = np.asarray(features)\n",
        "#     #joint_scaler = joint_scaler.reshape(-1, features_num)\n",
        "#     #scaler2 = scaler2.fit(joint_scaler) \n",
        "#     features = scaler2.transform(features)\n",
        "\n",
        "#     targets = data.iloc[:,:3]\n",
        "#     targets = np.asarray(targets)\n",
        "#     targets = targets.reshape(-1,3)\n",
        "    \n",
        "#     for i in range(len(features)):           \n",
        "   \n",
        "#         X =(features[:i+1])\n",
        "#         an_array = np.array(X)\n",
        "#         shape = np.shape(X)\n",
        "#         temp = np.zeros((seq_size, features_num))\n",
        "#         temp[(seq_size-shape[0]):,:shape[1]] = an_array\n",
        "#         X_test.append(temp)\n",
        "#         y_test = targets\n",
        "\n",
        "#  #############################################\n",
        "\n",
        "#     #scaler = StandardScaler()\n",
        "#     #force_scaler = np.asarray(y_test)\n",
        "#     #force_scaler = force_scaler.reshape(-1,3)\n",
        "#     #scaler = scaler.fit(force_scaler)    \n",
        "#     y_test = scaler.transform(y_test)\n",
        "\n",
        "\n",
        "# ###############################################\n",
        "\n",
        "#     X_train = torch.cuda.FloatTensor(X_train) # Change data to tensors\n",
        "#     y_train = torch.cuda.FloatTensor(y_train)\n",
        "#     X_test = torch.cuda.FloatTensor(X_test)\n",
        "#     y_test = torch.cuda.FloatTensor(y_test)\n",
        "    \n",
        "#     #del targets, features, data, temp, an_array\n",
        "    \n",
        "#     #print(data)\n",
        "#     #print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "#     return(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thk1hwULis6G",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cv2RZ52m-zQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "tot_losses = []\n",
        "tot_test_losses = []\n",
        "\n",
        "seq_length = get_seq_length() # when using zero padding, this seq_length is a bit redundent but still has to match the zero's size.\n",
        "\n",
        "model = ForcePredictor(\n",
        "      n_features=get_features(), \n",
        "      n_hidden= get_hidden(), #32, #64\n",
        "      seq_len=seq_length, \n",
        "      n_layers=2\n",
        "    )\n",
        "\n",
        "model, train_hist, test_hist, optimizer, epochs, loss = train_model(model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JQbLodYJ40g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Saving model\")\n",
        "model_num = model_number()\n",
        "model_save_name = f'model_params{model_num}_vlast.pt'\n",
        "torch.save({\n",
        "   # 'epoch': epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss':loss,},\n",
        "    f\"/content/drive/My Drive/PhD/PhD/lstm/{model_save_name}\" \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "u9wQYHgS-zQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.pyplot import figure\n",
        "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "figure(figsize=(20,4))\n",
        "plt.plot(train_hist[1000:], label=\"Training loss\")\n",
        "\n",
        "#plt.plot(test_hist, label=\"Test loss\")\n",
        "#plt.ylim((0, 5))\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnwajR7M-zQO",
        "colab_type": "text"
      },
      "source": [
        "RL Controller Predictor"
      ]
    }
  ]
}