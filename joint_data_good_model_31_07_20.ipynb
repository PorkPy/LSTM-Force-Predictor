{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "joint_data_good_model_31-07-20.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PorkPy/LSTM-Force-Predictor/blob/master/joint_data_good_model_31_07_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX11X6HgJtC7",
        "colab_type": "text"
      },
      "source": [
        "# Mount google Drive\n",
        "Allow Colab to access your Google Drive for saving models and other things. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtEIpTbrfxxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPmlOUyqKZUA",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mzCKBtLA-zOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "% reset -f\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import matplotlib.ticker as plticker\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import norm\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "from subprocess import call\n",
        "import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "## Set random seed for numpy and Torch\n",
        "RANDOM_SEED = 0\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK0q99L_KuNe",
        "colab_type": "text"
      },
      "source": [
        "# Version Data\n",
        "This block return Python, PyTorch and CUDA versions, plus, the number of GPUs available as well as any prosesses currently running. The gpu data can be copyed to elsewhere in the code, such as in the training loop to check gpu prosesses and memory usage. \n",
        "\n",
        "versions used here are:\n",
        "\n",
        "    Python VERSION: 3.6.9 \n",
        "    pyTorch VERSION: 1.5.1+cu101\n",
        "    CUDA VERSION:\n",
        "        Cuda compilation tools, release 10.1, V10.1.243\n",
        "        CUDNN VERSION: 7603\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKX3q9H0_RZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('__Python VERSION:', sys.version)\n",
        "print('__pyTorch VERSION:', torch.__version__)\n",
        "print('__CUDA VERSION')\n",
        "! nvcc --version\n",
        "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
        "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
        "print('__Devices')\n",
        "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
        "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
        "print('Available devices ', torch.cuda.device_count())\n",
        "print('Current cuda device ', torch.cuda.current_device())\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5WXvVoh-zOu",
        "colab_type": "text"
      },
      "source": [
        "Check CUDA is available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJf9cG7o-zOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW5oLv2AQESC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# Input Model Hyper-parameter settings\n",
        "---\n",
        "Here, one can stipulate the model number which is used to make a unique directory in your Google drive, along with sub-directories containing all the testing metrics. \n",
        "\n",
        "The hyper-parameters that can be changed here are:\n",
        "<br>\n",
        "<br>\n",
        "**lr** = learning rate.\n",
        "<br>\n",
        "**fc** = number of fully connected layers bolted on to the back of the LSTM layers.\n",
        "<br>\n",
        "**path** = your desiered path to save and load models in Google Drive. \n",
        "<br>\n",
        "**hidden** = number of hidden LSTM layers.\n",
        "<br>\n",
        "**epochs** = number of times to run through the whole training data.\n",
        "<br>\n",
        "**seq_length** = sequence length = the number of samples in each slice of a trajectory.\n",
        "<br>\n",
        "**feature_num** = number of features or independent variables in the data. \n",
        "<br>\n",
        "<br>\n",
        "Adjusting the number of features allows users to use a variable number of features or different types of data, such as joint data with 4 joints or Cartesian data with 6 Cartesian positions. \n",
        "<br>\n",
        "\n",
        "The model automatically adjusts itself depending on the number of layers and features required. \n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35dozZZC3Nut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''              ### MODEL HYPER-PARAMETER SETTINGS ###\n",
        "-------------------------------------------------------------------------------\n",
        "Here, the model hyper-parameters can be set, along with the save model path \n",
        "and the warm start model parameters path if using a pretrained model. \n",
        "'''\n",
        "torch.manual_seed(42)\n",
        "model_num  = '501'        ## Unique model number for saving new model.\n",
        "model_dir  = 'model501'   ## New directory name to save new model to. \n",
        "params     = '115_v0'     ## Pretrained model params to load...\n",
        "warm_start = False        ## ...and if to load them or not.\n",
        "seq_length = 250          ## The length of the trajectory slice trained on.\n",
        "epochs     = 101          ## Number of full passes through the whole dataset.\n",
        "hidden     = 60           ## Number of nodes in the LSTM layers.\n",
        "lr         = 0.0005       ## Learning rate.\n",
        "feature_num   =  4        ## 4 features for joint data, 6 features for cartesian data.\n",
        "fc         = 1            ## Number of fully connected layers. 1 or 2.\n",
        "random_seed = 0           ## Used to seed the random number generator for reproducibility.\n",
        "path       = f\"/content/drive/My Drive/PhD/PhD/lstm/{model_dir}/\" ## Save directory.\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "## I increased the batch size and lr by 1 order.\n",
        "\n",
        "## Functions to fetch hyper-parameters\n",
        "def model_number():\n",
        "    return model_num\n",
        "\n",
        "def load_params():\n",
        "    return params\n",
        "\n",
        "def model_directory():\n",
        "    return model_dir\n",
        "\n",
        "def get_seq_length():\n",
        "    return seq_length\n",
        "\n",
        "def get_epochs():\n",
        "    return epochs\n",
        "\n",
        "def get_warm_start():\n",
        "    return warm_start\n",
        "\n",
        "def get_hidden():\n",
        "    return hidden\n",
        "\n",
        "def get_lr():\n",
        "    return lr\n",
        "\n",
        "def get_path():\n",
        "    return path\n",
        "\n",
        "def get_features():\n",
        "    return feature_num\n",
        "\n",
        "def get_fc():\n",
        "    return fc\n",
        "\n",
        "def get_random_seed():\n",
        "    return random_seed\n",
        "\n",
        "## Dictionary with which to save paramers.\n",
        "param = {'Model Num':model_num, \n",
        "          'Seq Length': seq_length,\n",
        "          'Epochs': epochs,\n",
        "          'Warm Start': (warm_start),\n",
        "          'Hidden Size': hidden,\n",
        "          'Learning Rate': lr,\n",
        "          'features': feature_num, \n",
        "          'Num LSTM Layers':2,\n",
        "          'Num FC Layers':1,\n",
        "          'Data/Notes': 'full cart data, changed where zero_grad is, loss reporting/saving.'\n",
        "}\n",
        "\n",
        "## Create new directory in perent directory to save parameters.\n",
        "try:\n",
        "    os.makedirs(path)\n",
        "except OSError:\n",
        "    print (\"Creation of the directory %s failed\" % path)\n",
        "else:\n",
        "    print (\"Successfully created the directory %s \" % path)\n",
        "\n",
        "\n",
        "## create a pandas data frame of the model parameters and save to csv.\n",
        "param = pd.DataFrame(param, index=[0])\n",
        "param.to_csv(path + \"lstm_params.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCzMAAlzeTj3",
        "colab_type": "text"
      },
      "source": [
        "# Testing Loop\n",
        "---\n",
        "\n",
        "This block fetches the testing data and passes it through the model after a set number of training runs.\n",
        "<br>\n",
        "For each trajectory in the training data, three metrics are obtained:\n",
        "<br>\n",
        "<br>\n",
        "**MAE** = Mean Absolute Error accross each trajectory, measured in Newtons.\n",
        "<br>\n",
        "**RMSD** = Root Mean Sqared Deviation. Similar to MAE, but sensitive to outliers.\n",
        "<br>\n",
        "**COV** = Coefficient of Variance = RMSD/mean of the dataset. Useful for comparing performance across different models and datasets.\n",
        "<br>\n",
        "<br>\n",
        "For each trained model, we find the **Grand Mean** or mean of means, **Standard Deviation** and **Max Value** over all the testing trajectories.\n",
        "<br>\n",
        "\n",
        "**PDF** or Probability Density Function and **Gaussian Distribution** plots of each metric are then created to give an intuative view of the metrics used.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "O7ZKYszy-zQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lighten_color(color, amount=0.5):\n",
        "    import matplotlib.colors as mc\n",
        "    import colorsys\n",
        "    try:\n",
        "        c = mc.cnames[color]\n",
        "    except:\n",
        "        c = color\n",
        "    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n",
        "    return colorsys.hls_to_rgb(c[0], max(0, min(1, amount * c[1])), c[2])\n",
        "\n",
        "def tests(model_name):\n",
        "    \n",
        "    ## fetch parameters\n",
        "    seq_len = get_seq_length()\n",
        "    model_name = model_name\n",
        "    model_dir = model_directory()\n",
        "    path = get_path()\n",
        "    features_num = get_features()\n",
        "\n",
        "    \n",
        "    ## Create new directory in perent directory\n",
        "    path = path + f\"{model_name}/\"\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError:\n",
        "        print (\"Creation of the directory %s failed\" % path)\n",
        "    else:\n",
        "        print (\"Successfully created the directory %s \" % path)\n",
        "\n",
        "    stats_list = []\n",
        "    pdf = PdfPages(path + f\"testing_traj_pics_{model_name}.pdf\")\n",
        "    fig = plt.figure()\n",
        "\n",
        "    \n",
        "    #model = model.to(device)\n",
        "    #print(model)\n",
        "    #print(\"testing weights\", model.linear1.weight.data) # Check weights are being updated.\n",
        "\n",
        "    #model.reset_hidden_state()\n",
        "    for traj in range(len(test_batches)):\n",
        "        model.reset_hidden_state()\n",
        "        whole_traj = []\n",
        "        whole_true = []\n",
        "\n",
        "        for start_seq in range(int(1000/seq_length)): \n",
        "            start_seqx = start_seq*seq_length ## get the next sequence start position\n",
        "            #model.reset_hidden_state()\n",
        "\n",
        "            Xtest, ytest = get_test_batch(traj, start_seqx)\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                \n",
        "                x = iter(Xtest)\n",
        "                test_seq = Xtest[0].reshape(-1,seq_len,features_num)#.reshape(1,200,4) # input first sequence from trajectory/batch\n",
        "                preds = [] # create a list to store predictions.\n",
        "                for i in range(len(Xtest)): # for each sequence i in the trajectory,\n",
        "                    y_test_pred = model(test_seq).to(device)# send sequence to model,\n",
        "                    pred = torch.flatten(y_test_pred).cpu() # reshape the model output,\n",
        "                    preds.append(np.asarray(pred)) # and append to the list of predictions - preds.\n",
        "                    new_seq = next(x).reshape(-1,seq_len,features_num)#.reshape(1,200,4) # Change sequence to the next one in the list.\n",
        "                    test_seq = torch.cuda.FloatTensor(new_seq).view(1, seq_len, -1) # change sequence to a torch Tensor\n",
        "            whole_traj.append(preds)\n",
        "            whole_true.append(ytest)\n",
        "        whole_true = np.array(whole_true).reshape(-1,)\n",
        "        ## rescale the output predictions\n",
        "        preds = target_scaler.inverse_transform(whole_traj).reshape(-1,3)\n",
        "        ## Vector summation - the vector sum of the 3 output predictions\n",
        "        force_vec = np.sqrt((preds[:,0]**2)+(preds[:,1]**2)+(preds[:,2]**2))\n",
        "        \n",
        "        ytest = whole_true\n",
        "        preds = force_vec ## reset name to comply with existing code.\n",
        "        #display(force_vec)\n",
        "        \n",
        "        #Mean Absolute Error\n",
        "        MAE_list = []\n",
        "        for i,j in zip(preds, ytest):\n",
        "            error = np.abs(i-j)\n",
        "            MAE_list.append(error)\n",
        "        MAE = float(\"{:.3f}\".format(np.mean(MAE_list)))\n",
        "        #print(\"MAE\",\"{:.3f}\".format(MAE),'N')\n",
        "\n",
        "        # Coefficient of Variance\n",
        "        mean = np.mean(data.iloc[:,-1]) # mean of all dependent variables.\n",
        "        cov_list = []\n",
        "        for i,j in zip(preds, ytest):\n",
        "            sq_dev = (i-j)**2\n",
        "            cov_list.append(sq_dev)    \n",
        "        MSD = np.mean(cov_list) # mean square deviation\n",
        "        RMSD = np.sqrt(MSD) # root mean square deviation\n",
        "        cov = RMSD/mean # coefficient of variance\n",
        "        RMSD = float(\"{:.3f}\".format(RMSD))\n",
        "        cov =  float(\"{:.3f}\".format(cov))\n",
        "        #print(\"COV:\",\"{:.3f}\".format(cov))\n",
        "        \n",
        "    \n",
        "        my_dict = {'Trajectory':traj,\n",
        "                'MAE': MAE, \n",
        "                'RMSD':RMSD,\n",
        "                'cov': cov, # Used to normalise the RMSD accross all the data\n",
        "        }\n",
        "        stats_list.append(my_dict)\n",
        "\n",
        "        # Plot forces\n",
        "        predicted_cases = preds\n",
        "        true_cases = ytest\n",
        "        # Add title and axis names\n",
        "        plt.title(f'Force Trajectory {traj}');\n",
        "        plt.xlabel('Sample num');\n",
        "        plt.ylabel('Force (N)');\n",
        "        plt.tight_layout();\n",
        "        plt.grid(True)\n",
        "        plt.ylim(-1, 70)\n",
        "        plt.plot(true_cases, color=lighten_color('b', 1.7), linewidth=3.0, label='Real Force');\n",
        "        plt.plot(predicted_cases, color=lighten_color('r', 1.0), linewidth=1.0, label='Predicted Force');\n",
        "        plt.legend(loc=2, prop={'size': 6})\n",
        "\n",
        "        # save the current figure\n",
        "        pdf.savefig(fig);\n",
        "        # destroy the current figure\n",
        "        plt.clf()\n",
        "\n",
        "    pdf.close()\n",
        "    stats_list = pd.DataFrame(stats_list)\n",
        "    return stats_list\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def stats(stats_list2, model_name):\n",
        "    \n",
        " \n",
        "    ## Get the mean of MAE, RMSD and cov.\n",
        "    mean_list = {\n",
        "                'MAE' :float(\"{:.3f}\".format(np.mean(stats_list2['MAE']))),\n",
        "                'RMSD':float(\"{:.3f}\".format(np.mean(stats_list2['RMSD']))),\n",
        "                'cov' :float(\"{:.3f}\".format(np.mean(stats_list2['cov'])))\n",
        "    }\n",
        "    ## Get the std-dev of MAE, RMSD and cov.\n",
        "    std_dev = {\n",
        "                'MAE' :float(\"{:.3f}\".format(np.std(stats_list2['MAE']))),\n",
        "                'RMSD':float(\"{:.3f}\".format(np.std(stats_list2['RMSD']))),\n",
        "                'cov' :float(\"{:.3f}\".format(np.std(stats_list2['cov'])))\n",
        "    }\n",
        "    ## Get the max value of MAE, RMSD and cov.\n",
        "    max_list = {\n",
        "                'MAE' :float(stats_list2['MAE'].max()),\n",
        "                'RMSD':float(stats_list2['RMSD'].max()),\n",
        "                'cov' :float(stats_list2['cov'].max())\n",
        "    }\n",
        "    ## append above dicts to stats_list2.\n",
        "    stats_list2 = stats_list2.append(mean_list, ignore_index=True).fillna('Grand Mean')\n",
        "    stats_list2 = stats_list2.append(std_dev, ignore_index=True).fillna('Standard Dev')\n",
        "    stats_list2 = stats_list2.append(max_list, ignore_index=True).fillna('Max Value')\n",
        "\n",
        "    #display(stats_list2)\n",
        "    path = get_path()\n",
        "    \n",
        "    ## Create new directory in perent directory\n",
        "    path = path + f\"{model_name}/\"\n",
        "    model_name = model_name\n",
        "\n",
        "    ## save stats_list as .csv in same directory as trajectory plots.\n",
        "    stats_list2.to_csv(path + f\"lstm_model_metrics_{model_name}.csv\", index=False)\n",
        "    return stats_list2\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "## Get a Gaussian Distribution of the MAE, RMSD and cov.\n",
        "def gauss_plot(stats_list2, name, error_type, num):\n",
        "    model_name = name\n",
        "    model_dir = model_directory()\n",
        "    path = get_path()\n",
        "    path = path + f\"{model_name}/\"\n",
        "\n",
        "    error = error_type ## Either; MAE, RMSD or cov.\n",
        "    pdf = PdfPages(path + f\"gauss_pic_{error}.pdf\")\n",
        "    fig = plt.figure()\n",
        "    \n",
        "    # define constants\n",
        "    mu = np.mean(stats_list2.iloc[:-3,num]) \n",
        "    sigma = np.sqrt(np.var(stats_list2.iloc[:-3,num]))\n",
        "    x1 = np.min(stats_list2.iloc[:-3,num])\n",
        "    x2 = np.max(stats_list2.iloc[:-3,num])\n",
        "    \n",
        "\n",
        "    # calculate the z-transform\n",
        "    z1 = ( x1 - mu ) / sigma\n",
        "    z2 = ( x2 - mu ) / sigma\n",
        "\n",
        "    x = np.arange(z1, z2, 0.001) # range of x in spec\n",
        "    x_all = np.arange(-10, 10, 0.001) # entire range of x, both in and out of spec\n",
        "    # mean = 0, stddev = 1, since Z-transform was calculated\n",
        "    y = norm.pdf(x,0,1);\n",
        "    y2 = norm.pdf(x_all,0,1);\n",
        "\n",
        "    # build the plot\n",
        "    fig, ax = plt.subplots(figsize=(9,6));\n",
        "    #plt.style.use('fivethirtyeight');\n",
        "    ax.plot(x_all,y2);\n",
        "\n",
        "    ax.fill_between(x,y,0, alpha=0.3, color='b');\n",
        "    ax.fill_between(x_all,y2,0, alpha=0.1);\n",
        "    ax.set_xlim([-4,4]);\n",
        "    ax.set_xlabel('# of Standard Deviations Outside the Mean');\n",
        "    ax.set_yticklabels([]);\n",
        "    ax.set_title(f'{model_name} {error} Std Dev');\n",
        "\n",
        "    plt.savefig('normal_curve.png', dpi=72, bbox_inches='tight');\n",
        "    plt.grid(True);\n",
        "    plt.tight_layout();\n",
        "    #plt.show()\n",
        "    # save the current figure\n",
        "    pdf.savefig(fig);\n",
        "    ## destroy the current figure\n",
        "    plt.clf()\n",
        "\n",
        "    # close the object\n",
        "    pdf.close()\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "## Get a PDF of the MAE, RMSD and cov.\n",
        "def prob_dist(stats_list2, name, error_type, num):    \n",
        "    model_name = name\n",
        "    model_dir = model_directory()\n",
        "    path = get_path()\n",
        "    path = path + f\"{model_name}/\"\n",
        "\n",
        "\n",
        "    error = error_type\n",
        "    pdf = PdfPages(path + f\"prob_dist_pic_{error}.pdf\")\n",
        "    fig = plt.figure()\n",
        "\n",
        "    import seaborn as sns\n",
        "    sns.distplot(stats_list2.iloc[:-3,num], color=\"darkslategrey\");\n",
        "    plt.xlabel(\"Force [newtons]\", labelpad=14);\n",
        "    plt.ylabel(\"Probability of Occurence\", labelpad=14);\n",
        "    plt.title(f\"Probability Distribution of {error}\", fontsize=20);\n",
        "    plt.grid(True);\n",
        "    plt.tight_layout();\n",
        "\n",
        "    #plt.show()\n",
        "    # save the current figure\n",
        "    pdf.savefig(fig);\n",
        "    # destroy the current figure\n",
        "    plt.clf()\n",
        "    plt.close('all') ## added this due to runtime warning, more than 20 figs open\n",
        "    # close the object\n",
        "    pdf.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv8OMkT7zvAe",
        "colab_type": "text"
      },
      "source": [
        "# Test Controller\n",
        "This block automates the execution of the testing code above by first calling tests() which generates the testing trajectory plots, but also calculates the MAE, RMSD and COV for each trajectory. The product of which is saved as a dictionary and returned.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2BXoG-jlHcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_runner(name):   \n",
        "    stats_df = tests(name) # Run tests on testing data and save generated plots to Google Drive\n",
        "    stats(stats_df, name) # Record stats and save to Google Drive\n",
        "    for i in range(1,4): # 1 to 3 = the colunms in the stats_list DataFrame\n",
        "        if i ==1:\n",
        "            error_type = 'MAE' # mean absolur error\n",
        "        elif i == 2:\n",
        "            error_type = 'RMSE' # root mean squared error\n",
        "        elif i == 3:\n",
        "            error_type = 'cov' # coefficient of variance\n",
        "\n",
        "        prob_dist(stats_df, name, error_type, i) # Gen prob_dist and save to GD\n",
        "        \n",
        "        gauss_plot(stats_df, name, error_type, i) # Gen Gauss plots and save to GD\n",
        "    print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLUhALEgroxu",
        "colab_type": "text"
      },
      "source": [
        "# LSTM Network\n",
        "---\n",
        "The network consists of two LSTM layers with sixty nodes each, plus one fully connected output layer; however, these hyper-parameters can be adjusted in the hyper-parameter section above. The network is able to automatically adjust the number of LSTM layers, nodes and fully connected layers between one and three when stipulated in the hyper-parameter block. \n",
        "<br>\n",
        "The reason for having adjustable fully connected layers here was because it was fount that the network could sometimes produce better models with more layers depending on the input data; however, the nature of the data has changed significantly since the start of this project, but it is still interesting to compare the performance when increasing the network complxity. \n",
        "<br>\n",
        "A lot of experiments were carried out on all the available activation functions in PyTorch and 'Leaky relu' was found to perform the best for this task.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSq64dXK-zO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ForcePredictor(nn.Module):\n",
        "\n",
        "    def __init__(self, n_features, n_hidden, seq_len, n_layers=2, ignore_zero=True):\n",
        "        super(ForcePredictor, self).__init__()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.device(\"cuda:0\")\n",
        "            print(\"Running on the GPU\")\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "            print(\"Running on CPU\")\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "        self.seq_len = seq_len\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "          input_size=n_features,\n",
        "          hidden_size=n_hidden,\n",
        "          num_layers=n_layers,\n",
        "          dropout=0.5)\n",
        "        \n",
        "        \n",
        "        fc = get_fc() ## get num of FC layers\n",
        "\n",
        "        if fc == 1:\n",
        "            self.linear1 = nn.Linear(in_features=n_hidden, out_features=3)\n",
        "            \n",
        "        elif fc == 2:\n",
        "            self.linear1 = nn.Linear(in_features=n_hidden, out_features=60)\n",
        "            self.linear2 = nn.Linear(in_features=60, out_features=3)\n",
        "\n",
        "        elif fc == 3:\n",
        "            self.linear1 = nn.Linear(in_features=n_hidden, out_features=60)\n",
        "            self.linear2 = nn.Linear(in_features=60, out_features=60)\n",
        "            self.linear3 = nn.Linear(in_features=60, out_features=3)\n",
        "        \n",
        "    def reset_hidden_state(self):\n",
        "        self.hidden = (\n",
        "            torch.zeros(self.n_layers, self.seq_len, self.n_hidden).to(device),\n",
        "            torch.zeros(self.n_layers, self.seq_len, self.n_hidden).to(device)\n",
        "        )\n",
        "\n",
        "    ## Forward Function.\n",
        "    \n",
        "    def forward(self, sequences):\n",
        "\n",
        "        '''                    ## Forward Method ##\n",
        "        -----------------------------------------------------------------------\n",
        "        This Method takes the input and passes it through each of the network \n",
        "        layers.\n",
        "        The number of fully connected layers the input gets passed through is\n",
        "        dependent on the number stipulated in the network hyper-parameters at\n",
        "        the beginning of the notebook.\n",
        "        ----------------------------------------------------------------------- \n",
        "        '''\n",
        "        \n",
        "        fc = get_fc() ## get num of FC layers\n",
        "\n",
        "        lstm_out, self.hidden = self.lstm(sequences.view(len(sequences), self.seq_len, -1),self.hidden)\n",
        "        last_time_step = lstm_out.view(self.seq_len, len(sequences), self.n_hidden)[-1]\n",
        "\n",
        "        if fc == 1:\n",
        "            y_pred = self.linear1(last_time_step)\n",
        "\n",
        "        if fc == 2:\n",
        "            y_pred = F.leaky_relu(self.linear1(last_time_step))\n",
        "            y_pred = self.linear2(y_pred)\n",
        "\n",
        "        if fc == 3:\n",
        "            y_pred = F.leaky_relu(self.linear1(last_time_step))\n",
        "            y_pred = F.leaky_relu(self.linear2(y_pred))\n",
        "            y_pred = self.linear3(y_pred)\n",
        "\n",
        "       \n",
        "\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4OcSy1rHxJv",
        "colab_type": "text"
      },
      "source": [
        "# Docstring Tester\n",
        "---\n",
        "This is just a little tester block to make sure the docstrings throughout the code are working as expected.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWjyr6PyEnHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_object = ForcePredictor(_,_,_)\n",
        "print(my_object.forward.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWusdxYDEmEk",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop\n",
        "---\n",
        "This is where the magic happens!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "894bcCIs-zPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model):\n",
        "    '''                   ## training Loop ##\n",
        "    ---------------------------------------------------------------------------\n",
        "\n",
        "    ---------------------------------------------------------------------------\n",
        "    '''\n",
        "    lr = get_lr()\n",
        "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)#0.0007 \n",
        "    print(\"learning rate =\", lr) \n",
        "    num_epochs = get_epochs() #1600 #600\n",
        "    path = get_path()\n",
        "    seq_length = get_seq_length()\n",
        "\n",
        "    #--------------------------------------------------------------------------\n",
        "    '''                   ## Get Model Parameters ##\n",
        "    ---------------------------------------------------------------------------\n",
        "    If you want to restart training from an earlier model, first - it should be\n",
        "    stipulated in the hyper-parameter setting at the beginning of the notebook\n",
        "    by setting 'warm_start' = true, and adding the path to the saved model \n",
        "    location.\n",
        "\n",
        "\n",
        "    '''\n",
        "    start_epoch = 0\n",
        "    warm_start = get_warm_start()\n",
        "    if warm_start == True:      \n",
        "        params = load_params() # model num and version num: 4_v100.\n",
        "        PATH = f\"/content/drive/My Drive/PhD/PhD/lstm/model_params{params}.pt\"     \n",
        "        checkpoint = torch.load(PATH)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        for state in optimizer.state.values():\n",
        "            for k, v in state.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    state[k] = v.cuda()\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        loss = checkpoint['loss']\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_hist = np.zeros(num_epochs + start_epoch)\n",
        "    test_hist = np.zeros(num_epochs + start_epoch)\n",
        "\n",
        "    print(model)\n",
        "    print(\"Starting Weights\", model.linear1.weight.data) # Check weights are being updated.\n",
        "    start_weights = model.linear1.weight.data.clone()\n",
        "\n",
        "    ## create new lists to save the losses over the whole training phase.\n",
        "    tot_losses = []\n",
        "    tot_test_losses = []\n",
        "    losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    ## Set time with which to reference elapsed time.\n",
        "    s1 = time.strftime('%H:%M:%S')\n",
        "\n",
        "    '''                   ## Start of Training Loop\n",
        "    ---------------------------------------------------------------------------\n",
        "    start_epoch = 0 unless using pre-trained model.\n",
        "    num_epochs is stipulated in model hyper-parameter settings.\n",
        "    \n",
        "    '''\n",
        "\n",
        "    for t in range(start_epoch, num_epochs):\n",
        "\n",
        "        ## Get elasped time for each epoch.\n",
        "        s2 = time.strftime('%H:%M:%S')\n",
        "        FMT = '%H:%M:%S'\n",
        "        tdelta = datetime.strptime(s2, FMT) - datetime.strptime(s1, FMT)\n",
        "        print(\"\\n Time Elapsed\", tdelta)\n",
        "       \n",
        "        for j in range(len(batches)): #-----------------------------------------## for each traj in training data.\n",
        "            if t % 10 != 0: \n",
        "                print(\"\\r\", f\"Epoch:{t} Batch:{j}\", end=\"\") #-----------## print epoch & batch for a visual reference during training. \n",
        "\n",
        "            model.reset_hidden_state() #----------------------------------------## reset hidden state for the start of each trajectory\n",
        "            #losses = [] #-------------------------------------------------------## reset sum of losses for each trajectoy. \n",
        "            optimizer.zero_grad() #---------------------------------------------## maybe keeping the gradiants over the whole trajectoy is a good idea??\n",
        "            ## Fetch n samples from each trajectory to train on before updatining network.\n",
        "            for start_seq in range(int(1000/seq_length)): #---------------------## 1000 samples divided by seq length returns num slices.\n",
        "                train_data, train_labels, _, _ = get_batches(j, start_seq*seq_length) ## dataloader- featch each slice of the traj at a time.\n",
        "                y_pred = model(train_data) #------------------------------------## make prediction on a slice/batch.\n",
        "                loss = loss_fn(y_pred.float(), train_labels) #------------------## loss for each slice/batch.\n",
        "                losses.append(loss.item()) #------------------------------------## append for all slices/batches == 1 trajectory.\n",
        "                \n",
        "                with torch.no_grad(): #-----------------------------------------## for some reason, the model breaks if this code block isn't here??? the model looses the map.\n",
        "                    random_pred = model(train_data) #---------------------------## Accessing the model seems to have some effect on the loss???\n",
        "                        \n",
        "                train_hist[t] = loss.item()\n",
        "                #optimizer.zero_grad() #----------------------------------------## moved zerograd to start of each trajectory. ### having zero_grad here produces strange peaks at start of trajectory\n",
        "                loss.backward() #-----------------------------------------------## Calculate the gradients of the loss wrt the network weights.\n",
        "                #model = model.to(device) #-------------------------------------## put model on GPU. ### shouldn't need this as already declared. \n",
        "                optimizer.step() #----------------------------------------------## Update network weights.\n",
        "            traj_loss = np.mean(losses) #---------------------------------------## average loss over each trajectory\n",
        "\n",
        "\n",
        "            model.reset_hidden_state() #----------------------------------------# Seperate training and testing hidden states. \n",
        "            #test_losses = []\n",
        "            for start_seq in range(int(1000/seq_length)): \n",
        "\n",
        "                #start_seqx = start_seq*seq_length ## get the next sequence start position\n",
        "                _, _, test_data, test_labels = get_batches(j, start_seq*seq_length)\n",
        "                with torch.no_grad():\n",
        "                    y_test_pred = model(test_data)\n",
        "                    test_loss = loss_fn(y_test_pred.float(), test_labels) ## loss for each batch\n",
        "                test_hist[t] = test_loss.item()\n",
        "                test_losses.append(test_loss.item()) ## append loss for each trajectory\n",
        "            traj_test_loss = np.mean(test_losses) ## average test loss over each tarjectory of n samples\n",
        "\n",
        "\n",
        "                ## The loss will look small (<1) but that's because we are not de-scaling the output. \n",
        "            if t % 10 == 0: \n",
        "                print(f'Epoch {t} {j} train loss: {traj_loss} test loss: {traj_test_loss}') ## display losses for each trajectory every 10 epochs.\n",
        "                \n",
        "        #tot_losses.append(loss.item()) ## save loss for each trajectory.\n",
        "        #tot_test_losses.append(test_loss.item())\n",
        "\n",
        "        ## Periodically save model and show training and testing loss\n",
        "        if t % 10 == 0:\n",
        "            print('Saving model', '\\n')\n",
        "            model_num = model_number()\n",
        "            model_save_name = f'model_params{model_num}_v{t}.pt'\n",
        "            torch.save({\n",
        "                'epoch': num_epochs,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss':loss,},\n",
        "                f\"/content/drive/My Drive/PhD/PhD/lstm/{model_save_name}\" \n",
        "            )\n",
        "\n",
        "            ## Show the training and testing losses during execution.\n",
        "            #from matplotlib.pyplot import figure\n",
        "            #figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "            #figure(figsize=(20,4))\n",
        "            #plt.plot(losses, label='Training Loss');\n",
        "            #plt.plot(test_losses, label='Testing Loss');\n",
        "            #plt.legend();\n",
        "            #plt.show()\n",
        "            #plt.pause(0.01)\n",
        "            mean_loss = np.mean(losses)\n",
        "            mean_test_loss = np.mean(test_losses)\n",
        "            print(\"\\n Average Loss\")\n",
        "            print(mean_loss)\n",
        "            print(\"\\n Average Test Loss\")\n",
        "            print(mean_test_loss,'\\n')\n",
        "            diff = mean_test_loss - mean_loss\n",
        "            print(\"Difference Between Training and Testing Losses\")\n",
        "            print(diff.item(),'\\n')\n",
        "            tot_losses.append(mean_loss)\n",
        "            tot_test_losses.append(mean_test_loss)\n",
        "        \n",
        "\n",
        "            name = f'model{model_num}_v{t}'\n",
        "            test_runner(name)\n",
        "            model.train() ## just in case it was left in .eval() mode.\n",
        "\n",
        "            model_dir = model_directory()\n",
        "\n",
        "            pdf = PdfPages(f\"/content/drive/My Drive/PhD/PhD/lstm/{model_dir}/loss.pdf\")\n",
        "            fig = plt.figure();\n",
        "\n",
        "            plt.plot(tot_losses, label='Training Loss');\n",
        "            plt.plot(tot_test_losses, label='Testing Loss');\n",
        "            plt.xlabel(\"100 epochs\", labelpad=14);\n",
        "            plt.ylabel(\"Loss\", labelpad=14);\n",
        "            plt.title(f\"Training and Testing Losses {model_dir}\", fontsize=20);\n",
        "            plt.grid(True);\n",
        "            plt.tight_layout();\n",
        "            plt.legend();\n",
        "\n",
        "            # save the current figure\n",
        "            pdf.savefig(fig);\n",
        "\n",
        "            figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "            plt.show()\n",
        "            plt.pause(0.01)\n",
        "\n",
        "            # destroy the current figure\n",
        "            plt.clf()\n",
        "            # close the object\n",
        "            pdf.close()\n",
        "\n",
        "        losses = []\n",
        "        test_losses = []\n",
        "\n",
        "    return model.eval(), train_hist, test_hist, optimizer, t, loss_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4nlT0C-9fUv",
        "colab_type": "text"
      },
      "source": [
        "# Import Data\n",
        "---\n",
        "This section did automatically selects which dataset to download based on the feature number selection made at the start. \n",
        "<br>\n",
        "However, it was descovered that the model can in fact utilise two or more cleaned datasets, picking feature columns as necessary, assuming the datasets are of the same original raw data.\n",
        "<br>\n",
        "For example, The Cartesian feature model uses the X, Y, Z, Rx, ry and Rz features of the Cartesian dataset, together with the Fx, Fy and Fz target features of the joints dataset.\n",
        "<br>\n",
        "Of course, these datasets could be combined and a lot of processed datasets based on the original raw data have been created and added to the gitbub repositiory; however, i think it's interesting to see how different datasets can be combined.\n",
        "<br>\n",
        "4 features for the joint data and 6 features for the Cartesian data. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtXzj7tF-zPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get num\n",
        "feature_num = get_features()\n",
        "url = 'https://raw.githubusercontent.com/PorkPy/LSTM-Force-Predictor/master/80k_data/cart_data_plus_rotation.csv'\n",
        "\n",
        "url2 = 'https://raw.githubusercontent.com/PorkPy/LSTM-Force-Predictor/master/80k_data/4_joints_3_force_1_forceVec.csv'\n",
        "\n",
        "data = pd.read_csv(url)\n",
        "data2 = pd.read_csv(url2)\n",
        "main_seq = data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efCoyWEDTXnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(data2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4lHYivtf_WY",
        "colab_type": "text"
      },
      "source": [
        "# Data Selection and Scaling\n",
        "---\n",
        "This block selects the desired features and targets from the datasets above and scales each section. The features and targets are scaled individually so that the output from the model can be de-scaled easily. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I218Ofy0ip-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = data2.iloc[:,3:7]\n",
        "feature_scaler = StandardScaler()\n",
        "features = feature_scaler.fit_transform(features)\n",
        "\n",
        "targets = data2.iloc[:,:3]\n",
        "target_scaler = StandardScaler()\n",
        "targets = target_scaler.fit_transform(targets)\n",
        "\n",
        "force_vec = pd.DataFrame(data2.iloc[:,-1])\n",
        "\n",
        "features = pd.DataFrame(features)\n",
        "targets = pd.DataFrame(targets)\n",
        "data = pd.concat([targets, features, force_vec], axis=1)\n",
        "\n",
        "data.columns = [['Fx', 'Fy', 'Fz', data2.columns[3], data2.columns[4], data2.columns[5], data2.columns[6], 'force vec']]\n",
        "print(data.columns)\n",
        "display(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TRvN1R4hK2q",
        "colab_type": "text"
      },
      "source": [
        "# Create Training and Testing Data Batches\n",
        "---\n",
        "This block takes the whole dataset and divides it up into individual trajectories that are 1000 samples each.\n",
        "<br>\n",
        "As the data were collected by performing different kineasthetic trajectories through the taskspace, and each trajectory was performed ten times; the trajectories are shuffled to remove corrolations between them. \n",
        "<br>\n",
        "Later, you will see how the trajectories themselves are further split into slices, and the model is trained on one slice at a time. Typical slice sizes have been 500, 250 and 125.\n",
        "<br>\n",
        "Because of this extra slicing, one may ask; why not just divide the dataset into 500, 250 or 125 sample trajectories to begine with?\n",
        "<br>\n",
        "We need to shuffle the trajectories to break up any corrolation between them, but doing this to all the slices would destroy the collolarion between slices belonging to a each trajectory. \n",
        "<br>\n",
        "Every batch of slices needs to be kelpt together during traing because they are still used as full trajectories by the model during training by not resetting the hidden states of the LSTM layers until all slices of a particular trajectory have been seen. \n",
        "<br>\n",
        "This means, that although the model has its parameters updated after seeing each slice, the history of previous slices still influence the training until the full trajectoy has passed through and the hidden states get reset.\n",
        "<br><br>\n",
        "Due to the current size of the dataset, The training set consists of sixty trajectories.\n",
        "<br>\n",
        "This leave a remander of eleven trajectories to test with; therefore, testing and validation sets use the same eleven trajectories; however,\n",
        "in order to compare training and testing loss as the model learns, the number of testing trajectory is inflated by copying them until they are the same length.\n",
        "<br>\n",
        "To overcome the problem of such a small testing set, cross validation is used to compare models and get an average loss overall. \n",
        "The cross validation is simply initiated by changing the random seed at the beginning of the notebook which reshuffels the data, resulting in largly different training and testing trajectories. \n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "IGY4CFZ9-zP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n=1000  ## num samples per trajectory/sequence.\n",
        "batchesx = [data[i:i + n] for i in range(0, len(data), n)] ## a list comprehension to build the data batches.\n",
        "print(len(batchesx))\n",
        "\n",
        "random.seed(get_random_seed())\n",
        "random.shuffle(batchesx)\n",
        "\n",
        "\n",
        "batches = batchesx[:60] ## Training batches up to the 60th sequence/trajectory.\n",
        "val_batches = batchesx[60:] ## Validation batches starting from the 60th sequence.\n",
        "\n",
        "## Append extra validation batches to even the number training and validation batches.\n",
        "## This is because the training loop performs a validation test on each iteration\n",
        "## and so always needs something to validate against.  \n",
        "while len(val_batches) < len(batches): \n",
        "    for i in batchesx[60:]:\n",
        "        val_batches.append(i)\n",
        "random.shuffle(val_batches)\n",
        "\n",
        "test_batches = batchesx[60:] ## Testing batches, same as validation batches, without the appendages.\n",
        "print(len(batches), len(val_batches), len(test_batches))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM22W0mO2SKA",
        "colab_type": "text"
      },
      "source": [
        "# Create Sequence Batches\n",
        "---\n",
        "It turns out that this sequencial data problem is more similar to natural language processing rather than stock pridiction because we have set sequences of data rather than continuous data, and because we need to make predictions righ from the start of each sequence not just at the end of them.\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "Natural language prosessing or nlp, has the problem of needing to input sequences of differing lengths (because words and sentences have different lengths) into an recurrant network that only accepts fixed length vectors. \n",
        "<br>\n",
        "To Overcome this, npl engineers use 'zero padding' which fills up a word or sentence vector with zeros until it is the correct size for the network. \n",
        "<br>\n",
        "This work is a little different but the zero padding technique helps a lot but allowing use to input every timestep of a trajectory from beginning to end while making predictions on every one. \n",
        "<br>\n",
        "To makes things a little mopre clear- the first datapoint of a trajectory is just that, one single datapoint, a sequence consisting of one timestep, but the final datapoint is part of a sequence containing a history of the previous 999 timesteps. \n",
        "<br>\n",
        "In order to fit these and all the other datapoints into the network,\n",
        "they need to be zeropadded to length 1000.\n",
        "The first sequence to be fed through the network will consist of one single datapoint and '999*num_features' zeros. \n",
        "<br>\n",
        "This means, that despite only having one single datapoint at the beginning of the trajectory, our LSTM-based model can still make a valid prediction of the force at that point.  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nehzUS3F-zQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(batch_num, start_seq):  \n",
        "    \n",
        "    seq_size = get_seq_length() # 1000 = full trajectories\n",
        "    features_num = get_features()\n",
        "\n",
        "    # random.seed(batch_num)\n",
        "    # random.shuffle(batches) # ive turned this off to test new cleaned data\n",
        "\n",
        "    # Randomise the fetching of new data to break the corrolation of training.\n",
        "    #print(batch_num)\n",
        "    #print(type(batches[batch_num])) \n",
        "    data = batches[batch_num].reset_index(drop=True)\n",
        "    data= data[start_seq:seq_size+start_seq]\n",
        "    ################################################\n",
        "\n",
        "    X_train = []\n",
        "    X_test = []\n",
        "    \n",
        "    if features_num == 4:\n",
        "        features = data[['joint_0', 'joint_2', 'joint_4', 'joint_5']]\n",
        "    else:\n",
        "        features = data[['x', 'y', 'z', 'Rx', 'Ry', 'Rz']]\n",
        "    features = np.asarray(features)\n",
        "\n",
        "\n",
        "    targets = data.iloc[:,:3]\n",
        "    targets = np.asarray(targets)\n",
        "    targets = targets.reshape(-1,3)\n",
        "\n",
        "    ## Create Zero-Padded Training Batches ##\n",
        "    for i in range(len(features)):           \n",
        "        \n",
        "        np.random.seed(42)\n",
        "       \n",
        "        X =(features[:i+1])\n",
        "        an_array = np.array(X)\n",
        "        shape = np.shape(X)\n",
        "        temp = np.zeros((seq_size, features_num))\n",
        "        temp[(seq_size-shape[0]):,:shape[1]] = an_array\n",
        "        X_train.append(temp)\n",
        "    y_train = targets\n",
        "    #--------------------------------------------------------------------------\n",
        "    \n",
        "    ## Validation Batches\n",
        "    data = val_batches[batch_num].reset_index(drop=True)\n",
        "    data= data[start_seq:seq_size+start_seq]\n",
        "\n",
        "    if features_num == 4:\n",
        "        features = data[['joint_0', 'joint_2', 'joint_4', 'joint_5']]\n",
        "    else:\n",
        "        features = data[['x', 'y', 'z', 'Rx', 'Ry', 'Rz']]\n",
        "    features = np.asarray(features)\n",
        "\n",
        "\n",
        "    targets = data.iloc[:,:3]\n",
        "    targets = np.asarray(targets)\n",
        "    targets = targets.reshape(-1,3)\n",
        "    \n",
        "    for i in range(len(features)):           \n",
        "   \n",
        "        X =(features[:i+1])\n",
        "        an_array = np.array(X)\n",
        "        shape = np.shape(X)\n",
        "        temp = np.zeros((seq_size, features_num))\n",
        "        temp[(seq_size-shape[0]):,:shape[1]] = an_array\n",
        "        X_test.append(temp)\n",
        "    y_test = targets\n",
        "##############################################################################\n",
        "    ## Change data to cuda tensors to use on gpu.\n",
        "    X_train = torch.cuda.FloatTensor(X_train) \n",
        "    y_train = torch.cuda.FloatTensor(y_train)\n",
        "    X_test = torch.cuda.FloatTensor(X_test)\n",
        "    y_test = torch.cuda.FloatTensor(y_test)\n",
        "    \n",
        "    del targets, features, data ## clear large volumn variables from mem\n",
        "    \n",
        "    #print(data)\n",
        "    #print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "    return(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqp4rH4m2BCR",
        "colab_type": "text"
      },
      "source": [
        "# Test Batches\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WabAxpcY-zP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_batch(batch_number, start_seq):\n",
        "    \n",
        "    seq_size = get_seq_length() ## 1000 for testing 50-100 for training\n",
        "    features_num = get_features()\n",
        "\n",
        "    X_test = []\n",
        "\n",
        "    data = test_batches[batch_number].reset_index(drop=True)\n",
        "    data= data[start_seq:seq_size+start_seq]\n",
        "\n",
        "    if features_num == 4:\n",
        "        features = data[['joint_0', 'joint_2', 'joint_4', 'joint_5']]\n",
        "    else:\n",
        "        features = data[['x', 'y', 'z', 'Rx', 'Ry', 'Rz']]\n",
        "    features = np.asarray(features)\n",
        "    y_test = data.iloc[:,-1]\n",
        "    \n",
        "    for i in range(len(features)):           \n",
        "   \n",
        "        X =(features[:i+1])\n",
        "        an_array = np.array(X)\n",
        "        shape = np.shape(X)\n",
        "        temp = np.zeros((seq_size, features_num))\n",
        "        temp[(seq_size-shape[0]):,:shape[1]] = an_array\n",
        "        X_test.append(temp)\n",
        "\n",
        "    \n",
        "    X_test = torch.cuda.FloatTensor(X_test)\n",
        "    return(X_test, y_test)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thk1hwULis6G",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfgLji9pGQ27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on CPU\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cv2RZ52m-zQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "tot_losses = []\n",
        "tot_test_losses = []\n",
        "\n",
        "seq_length = get_seq_length() # when using zero padding, this seq_length is a bit redundent but still has to match the zero's size.\n",
        "\n",
        "model = ForcePredictor(\n",
        "      n_features=get_features(), \n",
        "      n_hidden= get_hidden(), #32, #64\n",
        "      seq_len=seq_length, \n",
        "      n_layers=2\n",
        "    )\n",
        "\n",
        "model, train_hist, test_hist, optimizer, epochs, loss = train_model(model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JQbLodYJ40g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Saving model\")\n",
        "model_num = model_number()\n",
        "model_save_name = f'model_params{model_num}_vlast.pt'\n",
        "torch.save({\n",
        "   # 'epoch': epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss':loss,},\n",
        "    f\"/content/drive/My Drive/PhD/PhD/lstm/{model_save_name}\" \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "u9wQYHgS-zQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k');\n",
        "figure(figsize=(20,4));\n",
        "plt.plot(train_hist, label=\"Training Loss\");\n",
        "plt.plot(test_hist, label=\"Testing loss\");\n",
        "plt.title(\"Training and Testing Loss for Every Epoch\",fontsize=30)\n",
        "plt.grid(True);\n",
        "plt.tight_layout();\n",
        "plt.legend(loc=2, prop={'size': 15})\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnwajR7M-zQO",
        "colab_type": "text"
      },
      "source": [
        "RL Controller Predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0SQHEnsihAJ",
        "colab_type": "text"
      },
      "source": [
        "# MAE, Std-Dev and Max Values Plot\n",
        "---\n",
        "This code looks at all the metrics collected and plots the results for easy reference. \n",
        "<br>\n",
        "This removes the need to go through each model's testing trajectories looking for the best fitting model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcHVV40pfLS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_file(mode_dir):\n",
        "    \n",
        "    means = []\n",
        "    std_dev = []\n",
        "    max = []\n",
        "    PATH =  path = f\"/content/drive/My Drive/PhD/PhD/lstm/{model_dir}/*/*.csv\"\n",
        "    for file in glob.glob(PATH):\n",
        "        w = pd.read_csv(file, low_memory=False)\n",
        "        x = w.iloc[-3,1]\n",
        "        y = w.iloc[-2,1]\n",
        "        z = w.iloc[-1,1]\n",
        "        \n",
        "        means.append(x)\n",
        "        std_dev.append(y)\n",
        "        max.append(z)\n",
        "    return means, std_dev, max\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9ksl_GGfM6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_num = model_number()\n",
        "\n",
        "means, std_dev, max = find_file(f'model{model_num}')\n",
        "\n",
        "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k');\n",
        "figure(figsize=(20,4));\n",
        "fig = plt.figure();\n",
        "plt.plot(means, label='MAE');\n",
        "plt.plot(std_dev, label='std Dev');\n",
        "plt.plot(max, label='Max Values');\n",
        "plt.xlabel(\"x10 epochs\", labelpad=14);\n",
        "plt.ylabel(\"Error (N)\", labelpad=14);\n",
        "plt.title(f\"Testing Error {model_dir}\", fontsize=20);\n",
        "\n",
        "plt.grid(True);\n",
        "plt.legend();\n",
        "plt.tight_layout();\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTlkGAyffU1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pdf = PdfPages(f\"/content/drive/My Drive/PhD/PhD/lstm/{model_dir}/error.pdf\")\n",
        "\n",
        "# save the current figure\n",
        "pdf.savefig(fig);\n",
        "# destroy the current figure\n",
        "plt.clf()\n",
        "# close the object\n",
        "pdf.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}